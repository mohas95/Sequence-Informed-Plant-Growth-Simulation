{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6850313f-1ef5-4464-b8c1-834cebd1dd36",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74e4227c-36c8-4c23-abd1-9ad948c7e943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from utils import GardynDataset, envDataset, tensor_toImage, initiate_dir, save_model_config, load_model_config, imshow_batch, denormalize_image_tensor, normalize_image_tensor,calculate_fid, save_metrics, images_to_video\n",
    "torch.set_printoptions(precision=4, threshold=10, edgeitems=10, linewidth=100)\n",
    "from scipy import linalg\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, threshold=10, edgeitems=10, linewidth=100, suppress=True)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import gc\n",
    "\n",
    "import time, datetime, pytz\n",
    "str_format = '%Y%m%d%H%M%S'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Available Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb817ee-458f-4cba-b8aa-3214e465ea0b",
   "metadata": {},
   "source": [
    "# Defining data paths and playing around the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfbbe56d-98e4-44c4-99f2-0fe4d2a4853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## User input\n",
    "data_dir = './data/'\n",
    "data_file = 'data.csv'\n",
    "\n",
    "\n",
    "model_dir = f'./models/'\n",
    "model_name = 'SIPGS_baseline_NormalizedProModelbeta0.90_thisone_20240518084019'\n",
    "\n",
    "trial_name = 'test1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebf58aa2-51b3-435f-92fe-80792e2db7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './data/' already exists.\n",
      "Directory './models/' already exists.\n",
      "Directory './data/test1_simulated_output/' already exists.\n",
      "Directory './data/test1_simulated_output/' replaced.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./data/test1_simulated_output/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Paths\n",
    "initiate_dir(data_dir)\n",
    "initiate_dir(model_dir)\n",
    "\n",
    "##Output directories\n",
    "\n",
    "sim_out_dir = f'{data_dir}{trial_name}_simulated_output/'\n",
    "\n",
    "initiate_dir(sim_out_dir, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bd465c-74d1-406b-af20-4ac84abd1355",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1de68c67-15fe-44f8-a8fe-bdf7d9b55b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = envDataset(f'{data_dir}{data_file}', normalized =True, cumulative = True, params = ['Temperature_C', 'Humidity_percent', 'EC', 'PH', 'WaterTemp_C'])\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=1, shuffle=False)\n",
    "dataiter = iter(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70738d11-2381-4cc3-bc24-a32f5a39752a",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a147ab1f-f827-45db-baa6-660989cea2ae",
   "metadata": {},
   "source": [
    "## LSTM Feature Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cfa1337-4696-4c4b-b343-83fb2ea7e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM encoder for creating an embedded vector\n",
    "class LSTM_Feature_Encoder(nn.Module):\n",
    "    def __init__(self, feature_size, hidden_size = 128, num_layers=4, output_size= 64):\n",
    "        super(LSTM_Feature_Encoder, self).__init__()\n",
    "\n",
    "        self.params = {\n",
    "            'feature_size':feature_size,\n",
    "            'hidden_size':hidden_size,\n",
    "            'num_layers': num_layers,\n",
    "            'output_size': output_size\n",
    "        }\n",
    "\n",
    "        self.lstm = nn.LSTM(feature_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.Tanh()\n",
    "                               )\n",
    "        \n",
    "    def forward(self, padded_sequences, sequence_lengths):\n",
    "\n",
    "        packed_sequences = pack_padded_sequence(padded_sequences, sequence_lengths, batch_first = True, enforce_sorted=False)\n",
    "\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_sequences)\n",
    "\n",
    "        embedding = self.fc(hidden[-1])\n",
    "\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f42498f-bfbb-4af7-b69c-ca19d48cb946",
   "metadata": {},
   "source": [
    "## CVAE Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e41d26f-d43f-4f45-9f9a-9ae549a66fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE_Generator(nn.Module):\n",
    "    def __init__(self, img_shape, latent_size, conditional_size):\n",
    "        super(CVAE_Generator, self).__init__()\n",
    "\n",
    "        self.params = {\n",
    "            'img_shape':img_shape,\n",
    "            'latent_size':latent_size,\n",
    "            'conditional_size': conditional_size\n",
    "        }\n",
    "\n",
    "        self.img_shape = img_shape\n",
    "        self.conditional_size = conditional_size\n",
    "        self.input_size = self.img_shape[0] + self.conditional_size\n",
    "        self.latent_size = latent_size\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(self.input_size, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.hidden_shape, self.hidden_size = self._get_hidden_dimensions()\n",
    "\n",
    "        self.mu_fc = nn.Linear(self.hidden_size, self.latent_size)\n",
    "        self.logvar_fc = nn.Linear(self.hidden_size, self.latent_size)\n",
    "        \n",
    "        self.z_fc = nn.Linear(self.latent_size + self.conditional_size, self.hidden_size)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.hidden_shape[0], 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x, y):\n",
    "        x = x.view(x.size(0), self.conditional_size, 1, 1).repeat(1, 1, self.img_shape[1], self.img_shape[2])\n",
    "        x = torch.cat([y, x], dim=1)\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1) #Flatten encoder output for final Fully cunnected liear layer\n",
    "\n",
    "        mu, logvar = self.mu_fc(x), self.logvar_fc(x)\n",
    "        \n",
    "        return mu, logvar\n",
    "\n",
    "    def reparam(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "\n",
    "        z = mu + eps*std\n",
    "        \n",
    "        return z\n",
    "        \n",
    "    def decode(self, z, x):\n",
    "\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = torch.cat([z,x], dim=1)\n",
    "        x = self.z_fc(x)\n",
    "        x = x.view(x.size(0), self.hidden_shape[0], self.hidden_shape[1],self.hidden_shape[2])\n",
    "        y_hat = self.decoder(x)\n",
    "\n",
    "        return y_hat\n",
    "        \n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        mu, logvar = self.encode(x,y)\n",
    "        z = self.reparam(mu, logvar)\n",
    "        y_hat = self.decode(z,x)\n",
    "\n",
    "        return y_hat, mu, logvar\n",
    "\n",
    "    def _get_hidden_dimensions(self):\n",
    "\n",
    "        dummy_input = torch.randn(1, self.input_size, self.img_shape[1], self.img_shape[2])\n",
    "\n",
    "        x = self.encoder(dummy_input)\n",
    "        hidden_shape = x.size()[1:]\n",
    "        \n",
    "        x = x.view(x.size(0), -1) #flatten to (batch_size, flattened_dimension)\n",
    "\n",
    "        hidden_size = x.size(1)\n",
    "\n",
    "        return hidden_shape, hidden_size "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a660e081-7801-4b6d-98c1-13fb4a91aa95",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "026b99ec-2c18-45bb-9a6b-5c53e473f201",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.params = {\n",
    "            'img_shape':img_shape\n",
    "        }\n",
    "\n",
    "        self.img_shape = img_shape\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(self.img_shape[0], 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.hidden_shape, self.hidden_size= self._get_hidden_dimensions()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "            # nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, y):\n",
    "        x = self.cnn(y)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _get_hidden_dimensions(self):\n",
    "\n",
    "        dummy_input = torch.randn(1, self.img_shape[0], self.img_shape[1], self.img_shape[2])\n",
    "        x = self.cnn(dummy_input)\n",
    "        hidden_shape = x.size()[1:]\n",
    "        \n",
    "        x = x.view(x.size(0), -1) #flatten to (batch_size, flattened_dimension)\n",
    "        hidden_size = x.size(1)\n",
    "\n",
    "        return hidden_shape, hidden_size "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d3cb9f-e79a-4242-8738-83c42e9d0856",
   "metadata": {},
   "source": [
    "## import model .pt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2338048-8fbd-4990-b97a-8adad08fb372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_Feature_Encoder(\n",
      "  (lstm): LSTM(7, 128, num_layers=2, batch_first=True)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      ")\n",
      "CVAE_Generator(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(67, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (mu_fc): Linear(in_features=614400, out_features=128, bias=True)\n",
      "  (logvar_fc): Linear(in_features=614400, out_features=128, bias=True)\n",
      "  (z_fc): Linear(in_features=192, out_features=614400, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (cnn): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=614400, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=1, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "iteration_dir = f'{model_dir}{model_name}/'\n",
    "\n",
    "feature_encoder_params, feature_encoder_pt_file = load_model_config(f'{model_dir}{model_name}/feature_encoder.json')\n",
    "generator_params, generator_pt_file = load_model_config(f'{model_dir}{model_name}/generator.json')\n",
    "dicriminator_params, discriminator_pt_file= load_model_config(f'{model_dir}{model_name}/discriminator.json')\n",
    "\n",
    "feature_encoder = LSTM_Feature_Encoder(**feature_encoder_params).to(device)\n",
    "generator = CVAE_Generator(**generator_params).to(device)\n",
    "discriminator = Discriminator(**dicriminator_params).to(device)\n",
    "\n",
    "feature_encoder.load_state_dict(torch.load(feature_encoder_pt_file, map_location=device))\n",
    "generator.load_state_dict(torch.load(generator_pt_file, map_location=device))\n",
    "discriminator.load_state_dict(torch.load(discriminator_pt_file, map_location=device))\n",
    "\n",
    "print(feature_encoder)\n",
    "print(generator)\n",
    "print(discriminator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d159c8-f2ca-45d8-8d10-827eef5e5a1b",
   "metadata": {},
   "source": [
    "# Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7f911e7-a979-42f4-96c2-67837fb10fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612231eb9e7f485f84823c5690566da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11446 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    i=0\n",
    "    z_cs = torch.randn(1,generator.latent_size).to(device)\n",
    "    \n",
    "    feature_encoder.eval(), generator.eval(), discriminator.eval()\n",
    "    for label, sequence_length, ts in tqdm(data_loader):\n",
    "        \n",
    "        label = label.to(device)\n",
    "        \n",
    "        ## Encoder sequential feature data\n",
    "        feature_embedding = feature_encoder(label, sequence_length)\n",
    "\n",
    "        y_hat = generator.decode(z_cs, feature_embedding)\n",
    "\n",
    "        y_out = y_hat[0].detach().cpu().numpy()\n",
    "            \n",
    "        img = np.transpose(y_out, (1, 2, 0))\n",
    "        img = Image.fromarray((img*255).astype(np.uint8))\n",
    "        img.save(f'{sim_out_dir}{i}.jpg')\n",
    "\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104d4c0b-2675-4256-9a02-7507deec2db3",
   "metadata": {},
   "source": [
    "## Save Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12237abd-c283-443d-b073-881bfe645c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combining images to video at: ./data/test1.mp4\n",
      "Video ./data/test1.mp4 created successfully!\n"
     ]
    }
   ],
   "source": [
    "images_to_video(sim_out_dir, f'{data_dir}{trial_name}.mp4',120)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
